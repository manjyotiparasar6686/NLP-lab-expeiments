{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqTtYhz0NPkP",
        "outputId": "ce4af7bf-d0f0-4bf3-9726-2add5d33431e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "8ma9qJcVNb4H"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Himun2iNqNT",
        "outputId": "f7720f93-a55b-4e3f-af83-2be7f90d5e66"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "IQ3Vg6UuNueL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize(tokens):\n",
        "    vectorizer = CountVectorizer()\n",
        "    token_matrix = vectorizer.fit_transform([\" \".join(tokens), text])\n",
        "\n",
        "    return token_matrix\n",
        ""
      ],
      "metadata": {
        "id": "kdf9A37INzBB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize(text, top_n=2):\n",
        "\n",
        "    tokens = preprocess(text)\n",
        "\n",
        "    token_matrix = vectorize(tokens)\n",
        "\n",
        "    similarity = cosine_similarity(token_matrix)[0] # Calculate similarity between the tokenized text and each sentence\n",
        "    print(\"Vector Similarity Scores:\")\n",
        "    for i, score in enumerate(similarity):\n",
        "        print(f\"Sentence {i+1}: {score}\")\n",
        "\n",
        "    top_indices = similarity.argsort()[-top_n:][::-1] # Get indices of most similar sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "    summary = [sentences[i] for i in top_indices] # Extract most similar sentences\n",
        "\n",
        "    return ' '.join(summary)"
      ],
      "metadata": {
        "id": "jH8y_ffbN2_F"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "Artificial intelligence (AI), in its broadest sense, is intelligence exhibited by machines, particularly computer systems, as opposed to the natural intelligence of living beings. It is a field of research in computer science that develops and studies methods and software which enable machines to perceive their environment and uses learning and intelligence to take actions that maximize their chances of achieving defined goals.\"\"\"\n"
      ],
      "metadata": {
        "id": "4med-S7GN5xN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary = summarize(text)\n",
        "print(\"\\nSummary:\")\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdWnQMPDN8Iw",
        "outputId": "db701c02-8047-4b1f-e2a0-6864a50b597a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector Similarity Scores:\n",
            "Sentence 1: 0.9999999999999994\n",
            "Sentence 2: 0.7006490497453702\n",
            "\n",
            "Summary:\n",
            "\n",
            "Artificial intelligence (AI), in its broadest sense, is intelligence exhibited by machines, particularly computer systems, as opposed to the natural intelligence of living beings. It is a field of research in computer science that develops and studies methods and software which enable machines to perceive their environment and uses learning and intelligence to take actions that maximize their chances of achieving defined goals.\n"
          ]
        }
      ]
    }
  ]
}
